# 策略迭代、价值迭代算法——以Jack的租车问题为例

这是最基本的、最应该先学习的强化学习算法，即使它只适用于一些非常简单的问题和情况

提供了价值迭代和策略迭代以解决Jack的租车问题的演示示例，在`policy_iteration.py`和`value_iteration.py`。但是其运行时间**很长**，（可能算法优化还是没做好），使用了多进程并发计算。
⚠️请结合自身CPU情况，酌情设置参与计算的进程数`num_processes`

### 问题描述：**Jack's Car Rental Problem　(20 points)**

Jack有两个租车点，1号租车点和2号租车点，每个租车点最多可以停放20辆车。Jack每租出去一辆车可以获利10美金。每天租出去的车与收回的车的数量服从泊松分布𝜆^𝑛/𝑛! 𝑒^(−𝜆) 。每天夜里，Jack可以在两个租车点间进行车辆调配，每晚最多调配5辆车，且每调配一辆车花费2美金。
· 假设1号租车点租出去和收回的车辆服从λ=3的泊松分布，2号租车点租出去和收回的车辆数分别服从λ=4和λ=2的泊松分布。假设阻尼系数𝛾=0.09。

任务：
定义上述问题对应的MDP四元组，并采用策略迭代方法，求解**最优**调配策略以使得盈利最优化。


### 1. Model-Based: 策略迭代 v.s. 值迭代


可以使用一个表格简要总结策略迭代和价值迭代的共同点和使用情境：

- 相同点：

1. 都属于基于模型的强化学习(Model-Based Reinforcement Learning)方法，需要了解完整的环境模型（状态转移分布与奖励函数分布）
2. 适用于在**有限策略空间**中（即动作和状态空间均有限），寻找**最优策略**。（既然有限策略空间，自然使用的是确定性策略）
3. 都属于动态规划方法，时间差分方法，以迭代方式进行，每次迭代都需要对状态空间进行遍历。
4. 使用不同的方式估计Q函数

- 主要区别：

| 特性      | 策略迭代                | 值迭代                             |
|---------|---------------------|---------------------------------|
| 算法结构    | 分为策略评估和策略改进两个步骤     | 单步迭代，直接更新状态值函数                  |
| V含义     | 当前策略$V^{\pi}$       | 最优策略$V^{*}$                     |
| 收敛速度    | 较快（较少的迭代次数）         | 相对较慢（可能需要更多迭代次数）                |
| 单次迭代复杂度 | $O(\|S\|^3\|A\|^3)$ | $O(\|S\|^2\|A\|)$               |
| 最大迭代次数  | $\|A\|^{\|S\|}$     | 正比于$\frac{1}{1-\gamma}$和收敛阈值的对数 |
| 计算复杂度   | 每次迭代计算量大            | 单次迭代相对简单，但总迭代次数可能更多             |
| 适用场景    | 状态空间和动作空间较小，计算资源充足  | 状态空间较大，或策略评估计算成本高               |



