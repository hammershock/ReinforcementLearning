# q-learning与sarsa算法——以GridWorld格子世界为例

环境描述：
开始游戏的玩家（一个小黄豆）出生在格子世界的左上角，它每走一步需要消耗0.1个体力，触碰红线或视图走出边界会受到20点惩罚、走到右下角的终点绿色格子，获得10分奖励。

我们在本例中展示了基本的无模型(Model-Free)强化学习方法,
虽然我们在`grid_world.py`中详细定义了游戏的环境模型，（包括了所有的状态转移和奖励设置）， 对于本例中的智能体agent，它开始时对整体的环境模型一无所知
因此智能体需要自行在环境中探索，与环境不断交互才能提升对环境的认识，并改进自己的策略，由此引发了在探索(Exploration)与利用(
Exploitation)之间平衡的问题

### Model-free: Q-Learning v.s. SARSA

- 相同点

都属于无模型的强化学习（Model-Free Reinforcement Learning）方法，不需要模型即环境的动态特性（状态转移概率和奖励函数），且需要平衡Exploration&Exploitation
蒙特卡洛方法+动态规划,时间差分TD
增量式更新，在线学习
以不同的方式估计Q函数
最基础的版本适用于状态和动作均为离散的情况，使用Q表存储；但也使用神经网络近似Q函数，原理是类似的

- 主要区别

| 特性   | Q-Learning                                                                       | SARSA                                                                  |
|------|----------------------------------------------------------------------------------|------------------------------------------------------------------------|
| 算法类型 | 策略无关（Off-Policy）                                                                 | 同策略（On-Policy）                                                         |
| 更新规则 | $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$ | $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma Q(s', a') - Q(s, a)]$ |
| 探索策略 | 选择下一个动作 $a'$ 为使 $Q(s', a')$ 最大化的动作，无关实际采取的策略。                                    | 下一个动作 $a'$ 是根据当前策略实际选择的动作，受探索策略影响。                                     |
| 学习策略 | 学习最优策略 $Q^*$                                                                     | 学习与当前探索策略相匹配的 $Q^\pi$                                                  |
| 风险性  | 可能更冒险，因为它总是假设最优动作将会被选择                                                           | 较为谨慎，因为它考虑了实际将会采取的动作                                                   |
| 收敛性  | 在一定条件下保证收敛到最优策略                                                                  | 收敛到当前策略下的最优值函数                                                         |
| 适用场景 | 学习和评估最优策略, 最大化长期回报                                                               | 学习和评估实际策略,规避短期风险                                                       |
